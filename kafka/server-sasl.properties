broker.id=1
log.dirs=/home/linuxbrew/data/kafka/kafka-clustor/data
zookeeper.connect=172.16.254.29:21811,172.16.254.29:21812
zookeeper.connection.timeout.ms=6000
listeners=SASL_PLAINTEXT://:9092
advertised.listeners=SASL_PLAINTEXT://172.16.254.29:9092

#################认证#########################

security.inter.broker.protocol=SASL_PLAINTEXT
sasl.mechanism.inter.broker.protocol=PLAIN
sasl.enabled.mechanisms=PLAIN

authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer
#如果没有任何acl配置，topic对所有用户可见
allow.everyone.if.no.acl.found=false

#超级用户super.users=User:Bob;User:Alice
super.users=User:admin

listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

listener.name.sasl_plaintext.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="Admin111111" user_admin="Admin111111" user_alice="alice-pwd";

#################认证#########################

auto.create.topics.enable=false
#是否允许删除topic
delete.topic.enable=true

#官方建议，测试环境0，线上环境用默认值（3秒 3000）
group.initial.rebalance.delay.ms=0

#这是日志留存策略基于文件大小的参数。Kafka会定期删除那些大小超过该参数值的日志文件。
#默认值是-1，表示Kafka永远不会根据大小来删除日志。下面是1GB
#log.retention.bytes=1073741824

#日志留存的时长。这是个“三兄弟”，如果同时配置，优先选取ms的配置，minutes次之，hours最后。
#Kafka默认保存7天的数据，也就是说7天前的数据会被删除掉。目前Kafka优先基于消息中的时间戳来进行判断，
#如果没有指定时间戳才会根据日志文件的最新修改时间进行比较。这是日志留存策略基于时间的一组参数。
log.retention.hours=168


#与producer端的参数acks配合使用。只有在acks=all(或-1)时才有意义。它指定了必须要应答写请求的最小数量的副本。如果不能被满足，producer将会抛出NotEnoughReplicas或NotEnoughReplicasAfterAppend异常。该参数用于实现更好的消息持久性。
#如果acks设置成了all，那么该参数最佳取值区间是[1, replication.factor)。
min.insync.replicas=1

#是否开启unclean leader选举，即不在同步副本集合(in-sync replicas, ISR)中的副本也能被选举为leader。如果开启，就有可能丢失数据，所以最好还是关闭它。
unclean.leader.election.enable=false



# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000

# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000



# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3

# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600




# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=1


num.recovery.threads.per.data.dir=1


offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1




